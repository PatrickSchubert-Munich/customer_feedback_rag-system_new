import pandas as pd


def create_metadata_tool(collection):
    """
    Erstellt Metadata-Snapshot-Builder f√ºr Customer Manager.
    
    Da kein Metadata Analysis Agent mehr existiert, werden die Funktionen
    direkt (ohne @function_tool Decorator) implementiert und nur f√ºr den
    Snapshot-Build beim App-Start verwendet.
    
    Args:
        collection: ChromaDB Collection mit Metadaten
    
    Returns:
        callable: build_metadata_snapshot Funktion zur Snapshot-Erstellung
    """
    def get_all_metadata() -> list:
        """Alle Metadaten werden aus der Collection geladen."""
        data = collection.get(include=["metadatas"])
        meta_data = data["metadatas"]
        return meta_data

    def convert_metadata_to_dataframe(metadatas: list) -> pd.DataFrame:
        """Konvertiert die Liste der Metadaten-Dictionaries in ein pandas DataFrame."""
        return pd.DataFrame(metadatas)

    # Lade ALLE Metadaten aus der Collection
    metadatas_list = get_all_metadata()

    # Konvertiere die Metadaten-Liste in ein pandas DataFrame
    df_metadata = convert_metadata_to_dataframe(metadatas_list)

    # Stelle sicher, dass numerische Spalten korrekte Datentypen haben
    if not df_metadata.empty:
        # NPS sollte numerisch sein
        if "nps" in df_metadata.columns:
            df_metadata["nps"] = pd.to_numeric(df_metadata["nps"], errors="coerce")

        # Sentiment Score sollte float sein
        if "sentiment_score" in df_metadata.columns:
            df_metadata["sentiment_score"] = pd.to_numeric(
                df_metadata["sentiment_score"], errors="coerce"
            )

        # Token Count sollte int sein
        if "verbatim_token_count" in df_metadata.columns:
            df_metadata["verbatim_token_count"] = pd.to_numeric(
                df_metadata["verbatim_token_count"], errors="coerce"
            )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # METADATA EXTRACTION FUNCTIONS
    # Direkte Implementierung ohne @function_tool - nur f√ºr Snapshot-Build
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def get_unique_markets() -> str:
        """
        Liefert alle verf√ºgbaren M√§rkte, Regionen und L√§nder im Datensatz.

        Returns:
            str: Strukturierte Auflistung von:
                - M√§rkte: Vollst√§ndige Market-IDs (z.B. "C1-DE, C1-FR, CE-IT, ...")
                - Regionen: Business-Regionen-Codes (z.B. "C1, C3, C5, CE, IT, ...")
                - L√§nder: ISO 3166-1 Alpha-2 L√§ndercodes (z.B. "AT, DE, FR, IT, ...")
                oder "Keine Marktdaten verf√ºgbar." bei fehlenden Daten

        Examples:
            >>> get_unique_markets()
            "M√§rkte (5): C1-DE, C1-FR, CE-IT, C3-AT, IT-IT\\n\
Regionen (4): C1, C3, CE, IT\\n\
L√§nder ISO-Code (4): AT, DE, FR, IT"
        
        Notes:
            - M√§rkte werden am "-" gesplittet in Region (links) und Country (rechts)
            - Regionen sind Business-spezifische Codes (nicht geografisch)
            - L√§nder folgen ISO 3166-1 Alpha-2 Standard (2-Buchstaben-Codes)
        """
        lines = []
        
        # M√§rkte
        if "market" in df_metadata.columns:
            unique_markets = sorted(df_metadata["market"].dropna().unique())
            market_count = len(unique_markets)
            lines.append(f"M√§rkte ({market_count}): {', '.join(unique_markets)}")
        
        # Regionen
        if "region" in df_metadata.columns:
            unique_regions = sorted(df_metadata["region"].dropna().unique())
            # Filtere UNKNOWN heraus
            unique_regions = [r for r in unique_regions if r != "UNKNOWN"]
            region_count = len(unique_regions)
            lines.append(f"Regionen ({region_count}): {', '.join(unique_regions)}")
        
        # L√§nder (ISO-Format)
        if "country" in df_metadata.columns:
            unique_countries = sorted(df_metadata["country"].dropna().unique())
            # Filtere UNKNOWN heraus
            unique_countries = [c for c in unique_countries if c != "UNKNOWN"]
            country_count = len(unique_countries)
            lines.append(f"L√§nder ISO-Code ({country_count}): {', '.join(unique_countries)}")
        
        if not lines:
            return "Keine Marktdaten verf√ºgbar."
        
        return "\n".join(lines)

    def get_nps_statistics() -> str:
        """
        Liefert umfassende NPS-Statistiken des Datensatzes.

        Returns:
            str: Multi-Line String mit NPS-Analyse:
                - Anzahl Eintr√§ge
                - Durchschnitt (Mean)
                - Median
                - Range (Min-Max)
                - Kategorien-Verteilung (Detractor/Passive/Promoter mit Prozenten)
                Bei fehlenden Daten: "Keine NPS-Daten verf√ºgbar."

        Examples:
            >>> get_nps_statistics()
            "NPS-Statistiken (1500 Eintr√§ge):\\n‚Ä¢ Durchschnitt: 7.85\\n‚Ä¢ Median: 8.0\\n..."

        Notes:
            - NPS-Kategorien: Detractor (0-6), Passive (7-8), Promoter (9-10)
            - Prozente beziehen sich auf Gesamt-NPS-Eintr√§ge
        """
        if "nps" not in df_metadata.columns or df_metadata["nps"].isna().all():
            return "Keine NPS-Daten verf√ºgbar."

        nps_stats = df_metadata["nps"].describe()
        lines = []
        lines.append(f"NPS-Statistiken ({int(nps_stats['count'])} Eintr√§ge):")
        lines.append(f"‚Ä¢ Durchschnitt: {nps_stats['mean']:.2f}")
        lines.append(f"‚Ä¢ Median: {nps_stats['50%']:.1f}")
        lines.append(f"‚Ä¢ Range: {nps_stats['min']:.0f} - {nps_stats['max']:.0f}")

        # NPS-Kategorien berechnen
        nps_categories = (
            df_metadata["nps"]
            .apply(
                lambda x: "Detractor" if x <= 6 else "Passive" if x <= 8 else "Promoter"
            )
            .value_counts()
        )

        total = len(df_metadata["nps"].dropna())
        lines.append("‚Ä¢ Kategorien:")
        for category, count in nps_categories.items():
            percentage = (count / total) * 100
            lines.append(f"  - {category}: {count} ({percentage:.1f}%)")

        return "\n".join(lines)

    def get_sentiment_statistics() -> str:
        """
        Liefert Sentiment-Analysestatistiken des Datensatzes.

        Returns:
            str: Multi-Line String mit:
                - Sentiment-Labels-Verteilung (positiv/negativ/neutral mit Prozenten)
                - Sentiment-Scores (Durchschnitt, Range)
                Bei fehlenden Daten: "Keine Sentiment-Daten verf√ºgbar."

        Examples:
            >>> get_sentiment_statistics()
            "Sentiment-Verteilung (1500 Eintr√§ge):\\n‚Ä¢ positiv: 800 (53.3%)\\n..."
        """
        lines = []

        # Sentiment Labels
        if "sentiment_label" in df_metadata.columns:
            sentiment_counts = df_metadata["sentiment_label"].value_counts()
            total_sent = len(df_metadata["sentiment_label"].dropna())
            lines.append(f"Sentiment-Verteilung ({total_sent} Eintr√§ge):")
            for label, count in sentiment_counts.items():
                percentage = (count / total_sent) * 100
                lines.append(f"‚Ä¢ {label}: {count} ({percentage:.1f}%)")

        # Sentiment Scores
        if "sentiment_score" in df_metadata.columns:
            sentiment_stats = df_metadata["sentiment_score"].describe()
            lines.append("\nSentiment-Scores:")
            lines.append(f"‚Ä¢ Durchschnitt: {sentiment_stats['mean']:.3f}")
            lines.append(
                f"‚Ä¢ Range: {sentiment_stats['min']:.3f} bis {sentiment_stats['max']:.3f}"
            )

        return "\n".join(lines) if lines else "Keine Sentiment-Daten verf√ºgbar."

    def get_topic_statistics() -> str:
        """
        Liefert Topic-Klassifizierungs-Statistiken des Datensatzes.

        Returns:
            str: Multi-Line String mit:
                - Topic-Verteilung (mit Prozenten)
                - Durchschnittliche Confidence-Scores pro Topic
                Bei fehlenden Daten: "Keine Topic-Daten verf√ºgbar."

        Examples:
            >>> get_topic_statistics()
            "Topic-Verteilung (1500 Eintr√§ge):\\n‚Ä¢ Service: 450 (30.0%, √ò Confidence: 0.85)\\n..."
        """
        lines = []

        # Topic Labels
        if "topic" in df_metadata.columns:
            topic_counts = df_metadata["topic"].value_counts()
            total_topics = len(df_metadata["topic"].dropna())
            lines.append(f"Topic-Verteilung ({total_topics} Eintr√§ge):")
            
            for topic, count in topic_counts.items():
                percentage = (count / total_topics) * 100
                
                # Durchschnittliche Confidence f√ºr dieses Topic berechnen
                if "topic_confidence" in df_metadata.columns:
                    avg_confidence = df_metadata[df_metadata["topic"] == topic]["topic_confidence"].mean()
                    lines.append(f"‚Ä¢ {topic}: {count} ({percentage:.1f}%, √ò Confidence: {avg_confidence:.2f})")
                else:
                    lines.append(f"‚Ä¢ {topic}: {count} ({percentage:.1f}%)")

        return "\n".join(lines) if lines else "Keine Topic-Daten verf√ºgbar."

    def get_date_range() -> str:
        """
        Liefert den Zeitraum des Datensatzes mit L√ºcken-Erkennung.

        Returns:
            str: Zeitraum mit Start/End-Datum, Anzahl Tage, Eintr√§ge und L√ºcken-Info.
                 Format: "Zeitraum: 2024-01-01 bis 2024-12-31 (365 Tage Spanne, 1500 Eintr√§ge)"
                 Mit L√ºcken: "Zeitraum: 2024-01-01 bis 2024-12-31 (365 Tage Spanne, 1500 Eintr√§ge)
                              ‚ö†Ô∏è Achtung: Nicht alle Tage haben Daten! (150 Tage mit Eintr√§gen)"
                 Bei fehlenden Daten: "Keine Datumsdaten verf√ºgbar."

        Examples:
            >>> get_date_range()
            "Zeitraum: 2024-01-01 bis 2024-12-31 (365 Tage Spanne, 1500 Eintr√§ge)"
            
            >>> get_date_range()  # Mit L√ºcken
            "Zeitraum: 2024-01-01 bis 2024-12-31 (365 Tage Spanne, 1500 Eintr√§ge)
            ‚ö†Ô∏è Achtung: Nicht alle Tage haben Daten! Nur 150 von 365 Tagen haben Eintr√§ge."
        """
        if "date_str" not in df_metadata.columns:
            return "Keine Datumsdaten verf√ºgbar."

        dates = df_metadata["date_str"].dropna()
        if dates.empty:
            return "Keine g√ºltigen Datumsdaten verf√ºgbar."

        # Min/Max Datum
        date_min_str = dates.min()
        date_max_str = dates.max()
        
        # Parse und formatiere sch√∂n (ohne Zeitzone/Zeit wenn nicht n√∂tig)
        try:
            date_min_dt = pd.to_datetime(date_min_str)
            date_max_dt = pd.to_datetime(date_max_str)
            
            # Formatiere als YYYY-MM-DD (ohne Zeit)
            date_min = date_min_dt.strftime('%Y-%m-%d')
            date_max = date_max_dt.strftime('%Y-%m-%d')
            
            # Zeitspanne (Min bis Max)
            total_days_span = (date_max_dt - date_min_dt).days + 1  # +1 weil inklusiv
        except Exception:
            # Fallback: Nutze Original-Strings und berechne Spanne auf 1 Tag
            date_min = str(date_min_str)
            date_max = str(date_max_str)
            total_days_span = 1
        
        # Tats√§chliche Tage mit Daten (unique dates)
        unique_dates = pd.to_datetime(dates).dt.date.nunique()
        
        # Basis-Info
        result = f"Zeitraum: {date_min} bis {date_max} ({total_days_span} Tage Spanne, {len(dates)} Eintr√§ge)"
        
        # L√ºcken-Warnung wenn nicht alle Tage Daten haben
        coverage_percent = (unique_dates / total_days_span) * 100
        if coverage_percent < 90:  # Weniger als 90% Coverage
            result += f"\n‚ö†Ô∏è Achtung: Nicht alle Tage haben Daten! Nur {unique_dates} von {total_days_span} Tagen haben Eintr√§ge ({coverage_percent:.0f}% Abdeckung)."
        
        return result

    def get_verbatim_statistics() -> str:
        """
        Liefert Statistiken √ºber Feedback-Text-L√§ngen (Token-Anzahl).

        Returns:
            str: Multi-Line String mit:
                - Durchschnittliche Token-Anzahl
                - Median, Min, Max
                - L√§ngenverteilung (kurz/mittel/lang mit Prozenten)
                Bei fehlenden Daten: "Keine Token-Count-Daten verf√ºgbar."

        Examples:
            >>> get_verbatim_statistics()
            "Verbatim-Statistiken (1500 Texte):\\n‚Ä¢ Durchschnittliche L√§nge: 45.3 Token\\n..."

        Notes:
            - Kurz: ‚â§20 Token, Mittel: 21-100 Token, Lang: >100 Token
        """
        if "verbatim_token_count" not in df_metadata.columns:
            return "Keine Token-Count-Daten verf√ºgbar."

        token_stats = df_metadata["verbatim_token_count"].describe()
        lines = []
        lines.append(f"Verbatim-Statistiken ({int(token_stats['count'])} Texte):")
        lines.append(f"‚Ä¢ Durchschnittliche L√§nge: {token_stats['mean']:.1f} Token")
        lines.append(f"‚Ä¢ Median: {token_stats['50%']:.0f} Token")
        lines.append(f"‚Ä¢ K√ºrzester Text: {token_stats['min']:.0f} Token")
        lines.append(f"‚Ä¢ L√§ngster Text: {token_stats['max']:.0f} Token")

        # Kategorisierung nach Textl√§nge
        short_texts = (df_metadata["verbatim_token_count"] <= 20).sum()
        medium_texts = (
            (df_metadata["verbatim_token_count"] > 20)
            & (df_metadata["verbatim_token_count"] <= 100)
        ).sum()
        long_texts = (df_metadata["verbatim_token_count"] > 100).sum()

        total = len(df_metadata["verbatim_token_count"].dropna())
        lines.append("‚Ä¢ L√§ngenverteilung:")
        lines.append(
            f"  - Kurze Texte (‚â§20 Token): {short_texts} ({(short_texts / total) * 100:.1f}%)"
        )
        lines.append(
            f"  - Mittlere Texte (21-100 Token): {medium_texts} ({(medium_texts / total) * 100:.1f}%)"
        )
        lines.append(
            f"  - Lange Texte (>100 Token): {long_texts} ({(long_texts / total) * 100:.1f}%)"
        )

        return "\n".join(lines)

    def get_dataset_overview() -> str:
        """
        Liefert kompakte √úbersicht aller Datensatz-Kennzahlen.

        Returns:
            str: Multi-Line √úbersicht mit:
                - Gesamt-Anzahl Eintr√§ge
                - Anzahl/Liste M√§rkte
                - NPS-Durchschnitt
                - H√§ufigstes Sentiment
                - Zeitraum
                Komprimierte Darstellung f√ºr schnellen √úberblick.

        Examples:
            >>> get_dataset_overview()
            "üìä DATENSATZ-√úBERSICHT\\nGesamt: 1500 Eintr√§ge\\nüè¢ M√§rkte: 4 (C1-DE, ...)\\n..."
        """
        lines = []
        lines.append("üìä DATENSATZ-√úBERSICHT")
        lines.append(f"Gesamt: {len(df_metadata)} Eintr√§ge")
        lines.append("")

        # M√§rkte
        if "market" in df_metadata.columns:
            unique_markets = df_metadata["market"].nunique()
            lines.append(
                f"üè¢ M√§rkte: {unique_markets} ({', '.join(sorted(df_metadata['market'].dropna().unique()))})"
            )

        # NPS-Schnell√ºbersicht
        if "nps" in df_metadata.columns:
            nps_avg = df_metadata["nps"].mean()
            lines.append(f"‚≠ê NPS-Durchschnitt: {nps_avg:.2f}")

        # Sentiment-Schnell√ºbersicht
        if "sentiment_label" in df_metadata.columns:
            top_sentiment = (
                df_metadata["sentiment_label"].mode().iloc[0]
                if not df_metadata["sentiment_label"].empty
                else "N/A"
            )
            lines.append(f"üòä H√§ufigstes Sentiment: {top_sentiment}")

        # Zeitraum (nutze gleiche Formatierung wie get_date_range())
        if "date_str" in df_metadata.columns:
            dates = df_metadata["date_str"].dropna()
            if not dates.empty:
                try:
                    date_min_dt = pd.to_datetime(dates.min())
                    date_max_dt = pd.to_datetime(dates.max())
                    date_min = date_min_dt.strftime('%Y-%m-%d')
                    date_max = date_max_dt.strftime('%Y-%m-%d')
                    lines.append(f"üìÖ Zeitraum: {date_min} bis {date_max}")
                except Exception:
                    lines.append(f"üìÖ Zeitraum: {dates.min()} bis {dates.max()}")

        lines.append("")
        lines.append("üí° Verwende die spezifischen Tools f√ºr detaillierte Analysen.")

        return "\n".join(lines)

    def resolve_market_name(market_input: str) -> str:
        """
        Mappt User-Eingaben auf valide Datensatz-Marktnamen.
        
        Args:
            market_input (str): User-Eingabe f√ºr Markt.
                Unterst√ºtzt: K√ºrzel (DE, AT, CH, US), L√§ndernamen (Deutschland, Austria),
                exakte Market-IDs (C1-DE)

        Returns:
            str: Valider Market-Name aus Datensatz oder Fehlermeldung mit verf√ºgbaren M√§rkten.
                - Bei exakter √úbereinstimmung: Market-Name (z.B. "C1-DE")
                - Bei Partial Match: Erster Match oder Warnung bei Mehrdeutigkeit
                - Bei Fehler: "‚ùå Unbekannter Markt: ... Verf√ºgbare M√§rkte: ..."

        Examples:
            >>> resolve_market_name("DE")
            "C1-DE"
            
            >>> resolve_market_name("Deutschland")
            "C1-DE"
            
            >>> resolve_market_name("XYZ")
            "‚ùå Unbekannter Markt: 'XYZ'. Verf√ºgbare M√§rkte: C1-DE, C2-AT, ..."

        Notes:
            - Mapping: deutschland‚ÜíDE, √∂sterreich‚ÜíAT, schweiz‚ÜíCH, usa‚ÜíUS, etc.
            - Case-insensitive Matching
            - Bei Mehrdeutigkeit: Nutzt ersten Match mit Warnung
        """
        if "market" not in df_metadata.columns:
            return "‚ùå Keine Marktdaten verf√ºgbar."
        
        # Hole alle verf√ºgbaren M√§rkte
        available_markets = sorted(df_metadata["market"].dropna().unique())
        
        # Normalisiere Input
        market_lower = market_input.lower().strip()
        
        # 1. Exakte √úbereinstimmung (case-insensitive)
        for market in available_markets:
            if market.lower() == market_lower:
                return market
        
        # 2. Partial Match: User sagt "DE", wir finden "C1-DE" oder "C2-DE"
        matches = [m for m in available_markets if market_lower in m.lower()]
        
        if len(matches) == 1:
            # Eindeutige √úbereinstimmung
            return matches[0]
        elif len(matches) > 1:
            # Mehrere √úbereinstimmungen - w√§hle erste
            return f"‚ö†Ô∏è Mehrere M√§rkte gefunden: {', '.join(matches)}. Nutze ersten: {matches[0]}"
        
        # 3. L√§nder-Namen Mapping
        country_mapping = {
            "deutschland": "DE",
            "germany": "DE",
            "√∂sterreich": "AT",
            "oesterreich": "AT",
            "austria": "AT",
            "schweiz": "CH",
            "switzerland": "CH",
            "usa": "US",
            "united states": "US",
            "frankreich": "FR",
            "france": "FR",
            "italien": "IT",
            "italy": "IT",
            "spanien": "ES",
            "spain": "ES",
        }
        
        # Versuche L√§nder-Namen zu mappen
        if market_lower in country_mapping:
            country_code = country_mapping[market_lower]
            # Suche nach M√§rkten mit diesem L√§nder-Code
            matches = [m for m in available_markets if country_code.lower() in m.lower()]
            if len(matches) == 1:
                return matches[0]
            elif len(matches) > 1:
                return f"‚ö†Ô∏è Mehrere M√§rkte f√ºr {market_input}: {', '.join(matches)}. Nutze ersten: {matches[0]}"
        
        # 4. Keine √úbereinstimmung gefunden
        return f"‚ùå Unbekannter Markt: '{market_input}'. Verf√ºgbare M√§rkte: {', '.join(available_markets)}"

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SNAPSHOT BUILDER (einmalig beim App-Start)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def build_metadata_snapshot() -> dict:
        """
        Baut einen statischen Metadata-Snapshot f√ºr Customer Manager Instructions.
        
        Dieser Snapshot wird EINMALIG beim App-Start erstellt und direkt in die
        Instructions des Customer Manager embedded. Dadurch entfallen Runtime-Tool-Calls
        und der Manager kann Metadaten-Fragen direkt beantworten.
        
        Returns:
            dict: Snapshot mit allen Metadaten-Werten als formatierte Strings
                Keys: unique_markets, nps_statistics, sentiment_statistics,
                      topic_statistics, date_range, verbatim_statistics, 
                      dataset_overview, total_entries
        """
        return {
            "unique_markets": get_unique_markets(),
            "nps_statistics": get_nps_statistics(),
            "sentiment_statistics": get_sentiment_statistics(),
            "topic_statistics": get_topic_statistics(),
            "date_range": get_date_range(),
            "verbatim_statistics": get_verbatim_statistics(),
            "dataset_overview": get_dataset_overview(),
            "total_entries": str(len(df_metadata)),
        }

    # R√ºckgabe: Nur noch die build_snapshot Funktion (keine Tools mehr f√ºr Agents)
    return build_metadata_snapshot
