"""
Advanced Synthetic Data Generator for Customer Feedback - Enterprise Edition
=============================================================================

This advanced module generates high-quality synthetic customer feedback data with:
- Maximum diversity (age, gender, education, region)
- Complete anonymization with creative fake names
- Realistic variation and authenticity
- Seamless integration with real data
- Bias prevention through statistical controls
- Scalable for thousands of records
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
import string
import itertools
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')


@dataclass
class PersonaProfile:
    """
    Defines a customer persona with all relevant characteristics.
    
    Attributes:
        age_group (str): Age group of the persona
        gender (str): Gender of the persona
        education_level (str): Education level
        tech_affinity (str): Technology affinity level
        communication_style (str): Communication style preference
        typical_concerns (List[str]): List of typical concerns
        text_patterns (Dict[str, float]): Text pattern probabilities
        typo_probability (float): Probability of typos
        emoji_usage (float): Probability of emoji usage
        formality_level (float): Formality level (0-1)
    """
    age_group: str
    gender: str
    education_level: str
    tech_affinity: str
    communication_style: str
    typical_concerns: List[str]
    text_patterns: Dict[str, float]
    typo_probability: float
    emoji_usage: float
    formality_level: float


class BiasMonitor:
    """
    Monitors and prevents bias during data generation.
    Limit: No phrase/topic should be used more than 50 times.
    
    Args:
        max_repeats (int): Maximum number of times a phrase can be repeated. Defaults to 50
    """
    def __init__(self, max_repeats: int = 50):
        self.phrase_counter = defaultdict(int)
        self.topic_counter = defaultdict(int)
        self.persona_counter = defaultdict(int)
        self.market_counter = defaultdict(int)
        self.max_repeats = max_repeats
        
    def track_phrase(self, phrase: str) -> bool:
        """
        Checks if a phrase can still be used.
        
        Args:
            phrase (str): The phrase to check
            
        Returns:
            bool: True if phrase can be used, False if limit reached
        """
        if self.phrase_counter[phrase] >= self.max_repeats:
            return False  # Phrase zu oft verwendet
        self.phrase_counter[phrase] += 1
        return True
        
    def track_topic(self, topic: str) -> bool:
        """
        Checks topic balance.
        
        Args:
            topic (str): The topic to check
            
        Returns:
            bool: True if topic can be used, False if limit reached
        """
        if self.topic_counter[topic] >= self.max_repeats * 2:  # Topics dÃ¼rfen Ã¶fter vorkommen
            return False
        self.topic_counter[topic] += 1
        return True
        
    def get_report(self) -> Dict:
        """
        Returns a bias report.
        
        Returns:
            Dict: Dictionary containing bias statistics including most used phrases,
                  topic distribution, total phrases count, and warnings
        """
        return {
            'most_used_phrases': sorted(self.phrase_counter.items(), key=lambda x: x[1], reverse=True)[:10],
            'topic_distribution': dict(self.topic_counter),
            'total_phrases': len(self.phrase_counter),
            'warnings': [
                f"Phrase '{phrase}' verwendet {count}x (Limit: {self.max_repeats})" 
                for phrase, count in self.phrase_counter.items() 
                if count >= self.max_repeats
            ]
        }


class PhraseDiversifier:
    """
    Replaces monotonous phrases like "customer states" (2,331x in original!)
    with 100+ natural variations with usage tracking.
    """
    def __init__(self):
        self.usage_counter = defaultdict(int)
        
    def get_diverse_opening(self, sentiment: str, bias_monitor: BiasMonitor) -> str:
        """
        Gets an opening phrase that is NOT overused.
        If all options are exhausted, chooses the least used one.
        
        Args:
            sentiment (str): Sentiment type ('positiv', 'neutral', 'negativ')
            bias_monitor (BiasMonitor): BiasMonitor instance for tracking usage
            
        Returns:
            str: A diverse opening phrase
        """
        # Diese werden in _initialize_text_components() gesetzt
        # Hier nur Fallback fÃ¼r Ã¤ltere Code-Pfade
        fallback_openings = {
            'positiv': ["Sehr zufrieden mit", "Top Service bei", "Empfehlenswert ist"],
            'neutral': ["War heute bei", "Hatte einen Termin bei"],
            'negativ': ["EnttÃ¤uschend war", "Probleme gab es bei"]
        }
        
        options = fallback_openings.get(sentiment, fallback_openings['neutral'])
        
        # Finde verfÃ¼gbare Phrases
        available = [p for p in options if bias_monitor.track_phrase(p)]
        
        if available:
            chosen = random.choice(available)
        else:
            # Alle erschÃ¶pft - nimm die am wenigsten genutzte
            chosen = min(options, key=lambda p: self.usage_counter[p])
            self.usage_counter[chosen] += 1
            
        return chosen


class NPSSentimentCorrelator:
    """
    Establishes realistic correlation between NPS and sentiment.
    Based on analysis: Detractors (16.8%) mostly negative, Promoters (55.8%) mostly positive.
    """
    def __init__(self):
        # Realistische Verteilungen aus Analyse
        self.correlation_map = {
            'Detractor': {'positiv': 0.05, 'neutral': 0.25, 'negativ': 0.70},  # 70% negativ
            'Passive': {'positiv': 0.20, 'neutral': 0.60, 'negativ': 0.20},    # 60% neutral
            'Promoter': {'positiv': 0.70, 'neutral': 0.25, 'negativ': 0.05}    # 70% positiv
        }
        
    def get_realistic_sentiment(self, nps_category: str) -> str:
        """
        Returns sentiment based on NPS category.
        
        Args:
            nps_category (str): NPS category ('Detractor', 'Passive', 'Promoter')
            
        Returns:
            str: Sentiment label ('positiv', 'neutral', 'negativ')
        """
        probs = self.correlation_map[nps_category]
        return np.random.choice(['positiv', 'neutral', 'negativ'], p=list(probs.values()))


class TextLengthController:
    """
    Controls text lengths based on real data:
    Min: 1 word, Max: 361 words, Median: 21, Mean: 28.5
    """
    def __init__(self):
        # Aus Analyse der 17.884 echten Feedbacks
        self.min_words = 1
        self.max_words = 361
        self.median_words = 21
        self.mean_words = 28.5
        self.std_words = 25  # GeschÃ¤tzt aus Verteilung
        
    def get_realistic_length(self, sentiment: str) -> int:
        """
        Returns realistic word count.
        Negative feedback tends to be longer (detailed criticism).
        
        Args:
            sentiment (str): Sentiment type ('positiv', 'neutral', 'negativ')
            
        Returns:
            int: Realistic word count for the given sentiment
        """
        if sentiment == 'negativ':
            # Negative Feedbacks tendieren zu mehr Details
            target = int(np.random.normal(self.mean_words * 1.3, self.std_words))
        elif sentiment == 'positiv':
            # Positive Feedbacks oft kÃ¼rzer
            target = int(np.random.normal(self.mean_words * 0.8, self.std_words * 0.7))
        else:
            # Neutrale Feedbacks um Median
            target = int(np.random.normal(self.median_words, self.std_words * 0.6))
            
        # Clamp to realistic range
        return max(self.min_words, min(self.max_words, target))


class AdvancedSyntheticFeedbackGenerator:
    """
    Enterprise-grade generator for synthetic customer feedback data.
    
    Features:
    - Learns from real customer data
    - Complete anonymization with creative fake names
    - Demographic diversity
    - Realistic language patterns based on real feedback
    - Temporal dynamics
    - Bias control
    - Scalable for large datasets
    """
    
    def __init__(self, seed: int = 42, enable_fun_mode: bool = True):
        """
        Initializes the generator.
        
        Args:
            seed (int): Random seed for reproducibility. Defaults to 42
            enable_fun_mode (bool): Enables creative/fun names for dealerships. Defaults to True
        """
        np.random.seed(seed)
        random.seed(seed)
        
        self.enable_fun_mode = enable_fun_mode
        self.learned_patterns = {}  # Speichert Muster aus echten Daten
        
        # NEU: Initialisiere Bias-Prevention und Quality-Control Komponenten
        self.bias_monitor = BiasMonitor(max_repeats=50)
        self.phrase_diversifier = PhraseDiversifier()
        self.nps_sentiment_correlator = NPSSentimentCorrelator()
        self.text_length_controller = TextLengthController()
        
        # Initialisiere alle Komponenten
        self._initialize_fake_entities()
        self._initialize_personas()
        self._initialize_markets_and_regions()
        self._initialize_topics()  # NEU: Topics mit Gewichtungen
        self._initialize_text_components()
        self._initialize_temporal_patterns()
        
        # NPS-Kategorien mit EXAKTEN Verteilungen aus Analyse (17.884 Feedbacks)
        # Detractors: 16.8%, Passives: 27.4%, Promoters: 55.8%
        self.nps_distribution = {
            'Detractor': list(range(0, 7)),   # NPS 0-6
            'Passive': list(range(7, 9)),     # NPS 7-8
            'Promoter': list(range(9, 11))    # NPS 9-10
        }
        self.nps_weights = {
            'Detractor': 0.168,  # 16.8% aus Analyse
            'Passive': 0.274,    # 27.4% aus Analyse
            'Promoter': 0.558    # 55.8% aus Analyse
        }
    
    def learn_from_real_data(self, real_data_path: str):
        """
        Learns patterns from real customer data.
        
        Args:
            real_data_path (str): Path to CSV file with real data
            
        Returns:
            None
        """
        print("ðŸ“š Lerne aus echten Kundendaten...")
        
        try:
            df_real = pd.read_csv(real_data_path, encoding='utf-8')
        except:
            df_real = pd.read_csv(real_data_path, encoding='latin-1')
            
        # Extrahiere Verbatims
        if 'Verbatim' in df_real.columns:
            verbatims = df_real['Verbatim'].dropna().tolist()
            
            # Analysiere Muster
            self.learned_patterns = {
                'phrase_patterns': self._extract_phrase_patterns(verbatims),
                'length_distribution': self._analyze_text_lengths(verbatims),
                'topic_indicators': self._extract_topic_indicators(verbatims),
                'sentiment_phrases': self._extract_sentiment_phrases(df_real),
                'common_issues': self._extract_common_issues(verbatims)
            }
            
            print(f"   âœ“ {len(verbatims)} Verbatims analysiert")
            print(f"   âœ“ {len(self.learned_patterns['phrase_patterns'])} Phrasen-Muster extrahiert")
            print(f"   âœ“ {len(self.learned_patterns['common_issues'])} hÃ¤ufige Probleme identifiziert")
            
    def _extract_phrase_patterns(self, verbatims: List[str]) -> Dict:
        """
        Extracts recurring phrases from real verbatims.
        
        Args:
            verbatims (List[str]): List of verbatim texts
            
        Returns:
            Dict: Dictionary with categorized phrase patterns
        """
        
        patterns = {
            'wartezeit': [],
            'service': [],
            'kosten': [],
            'kommunikation': [],
            'werkstatt': []
        }
        
        # Keywords fÃ¼r Kategorisierung
        keywords = {
            'wartezeit': ['warten', 'wartezeit', 'stunden', 'minuten', 'lange', 'dauerte'],
            'service': ['mitarbeiter', 'freundlich', 'unfreundlich', 'personal', 'beratung'],
            'kosten': ['euro', 'rechnung', 'preis', 'kosten', 'teuer', 'zahlen'],
            'kommunikation': ['rÃ¼ckruf', 'angerufen', 'erreicht', 'information', 'mitgeteilt'],
            'werkstatt': ['reparatur', 'werkstatt', 'arbeit', 'fahrzeug', 'problem']
        }
        
        for verbatim in verbatims:
            if not isinstance(verbatim, str):
                continue
                
            verbatim_lower = verbatim.lower()
            sentences = verbatim.split('.')
            
            for sentence in sentences:
                if len(sentence) > 10:  # Nur sinnvolle SÃ¤tze
                    for category, words in keywords.items():
                        if any(word in sentence.lower() for word in words):
                            # Anonymisiere den Satz
                            anonymized = self._anonymize_sentence(sentence)
                            if anonymized and anonymized not in patterns[category]:
                                patterns[category].append(anonymized)
                                
        return patterns
    
    def _anonymize_sentence(self, sentence: str) -> str:
        """
        Anonymizes personal data in a sentence.
        
        Args:
            sentence (str): Sentence to anonymize
            
        Returns:
            str: Anonymized sentence
        """
        
        # Entferne spezifische Namen und Orte
        anonymized = sentence
        
        # Ersetze Uhrzeiten
        import re
        anonymized = re.sub(r'\d{1,2}:\d{2}', 'XX:XX', anonymized)
        anonymized = re.sub(r'\d{1,2}\.\d{2}', 'XX:XX', anonymized)
        
        # Ersetze Datumsangaben
        anonymized = re.sub(r'\d{1,2}\.\d{1,2}\.\d{2,4}', 'XX.XX.XXXX', anonymized)
        
        # Ersetze Kennzeichen
        anonymized = re.sub(r'[A-Z]{1,3}-[A-Z]{1,2}\s?\d{1,4}', 'XX-XX XXXX', anonymized)
        
        # Ersetze E-Mail-Adressen
        anonymized = re.sub(r'[\w\.-]+@[\w\.-]+', 'kunde@example.com', anonymized)
        
        # Ersetze Telefonnummern
        anonymized = re.sub(r'[\d\s\-\+\(\)]{10,}', 'TELEFONNUMMER', anonymized)
        
        # Ersetze groÃŸe Zahlen (vermutlich Preise oder IDs)
        anonymized = re.sub(r'\d{5,}', 'XXXXX', anonymized)
        
        return anonymized.strip()
    
    def _extract_common_issues(self, verbatims: List[str]) -> List[str]:
        """
        Extracts common issues from verbatims.
        
        Args:
            verbatims (List[str]): List of verbatim texts
            
        Returns:
            List[str]: List of common issues found
        """
        
        issues = []
        issue_patterns = [
            "wurde nicht zurÃ¼ckgerufen",
            "musste lange warten",
            "Termin nicht eingehalten",
            "Problem nicht behoben",
            "hÃ¶here Kosten als angegeben",
            "unfreundliches Personal",
            "keine Information erhalten",
            "Fahrzeug nicht fertig",
            "Ersatzteile nicht vorhanden",
            "schlechte Kommunikation"
        ]
        
        for pattern in issue_patterns:
            count = sum(1 for v in verbatims if isinstance(v, str) and pattern.lower() in v.lower())
            if count > 2:  # Nur hÃ¤ufige Probleme
                issues.append(pattern)
                
        return issues
    
    def _analyze_text_lengths(self, verbatims: List[str]) -> Dict:
        """
        Analyzes text length distribution.
        
        Args:
            verbatims (List[str]): List of verbatim texts
            
        Returns:
            Dict: Dictionary with min, max, mean, std, and quartiles of text lengths
        """
        
        lengths = [len(str(v).split()) for v in verbatims if isinstance(v, str)]
        
        return {
            'min': min(lengths) if lengths else 0,
            'max': max(lengths) if lengths else 0,
            'mean': np.mean(lengths) if lengths else 0,
            'std': np.std(lengths) if lengths else 0,
            'quartiles': np.percentile(lengths, [25, 50, 75]) if lengths else [0, 0, 0]
        }
    
    def _extract_topic_indicators(self, verbatims: List[str]) -> Dict:
        """
        Extracts topic indicators from verbatims.
        
        Args:
            verbatims (List[str]): List of verbatim texts
            
        Returns:
            Dict: Dictionary with topic counts
        """
        
        topics = {
            'Werkstatt': ['werkstatt', 'reparatur', 'inspektion', 'wartung'],
            'Service': ['service', 'beratung', 'mitarbeiter', 'personal'],
            'Terminvergabe': ['termin', 'wartezeit', 'pÃ¼nktlich', 'verspÃ¤tung'],
            'Kosten': ['preis', 'euro', 'rechnung', 'kosten', 'teuer'],
            'Kommunikation': ['anruf', 'rÃ¼ckruf', 'information', 'mitgeteilt']
        }
        
        topic_counts = {topic: 0 for topic in topics}
        
        for verbatim in verbatims:
            if not isinstance(verbatim, str):
                continue
            verbatim_lower = verbatim.lower()
            for topic, keywords in topics.items():
                if any(kw in verbatim_lower for kw in keywords):
                    topic_counts[topic] += 1
                    
        return topic_counts
    
    def _extract_sentiment_phrases(self, df_real: pd.DataFrame) -> Dict:
        """
        Extracts sentiment-specific phrases.
        
        Args:
            df_real (pd.DataFrame): DataFrame with real customer data
            
        Returns:
            Dict: Dictionary with sentiment-specific phrases
        """
        
        sentiment_phrases = {
            'positiv': [],
            'neutral': [],
            'negativ': []
        }
        
        if 'NPS' in df_real.columns and 'Verbatim' in df_real.columns:
            # Promoters (NPS 9-10) = meist positiv
            promoters = df_real[df_real['NPS'] >= 9]['Verbatim'].dropna()
            for verbatim in promoters:
                if isinstance(verbatim, str) and len(verbatim) > 20:
                    sentences = verbatim.split('.')
                    for s in sentences[:2]:  # Nur erste SÃ¤tze
                        if len(s) > 10:
                            sentiment_phrases['positiv'].append(self._anonymize_sentence(s))
            
            # Detractors (NPS 0-6) = meist negativ  
            detractors = df_real[df_real['NPS'] <= 6]['Verbatim'].dropna()
            for verbatim in detractors:
                if isinstance(verbatim, str) and len(verbatim) > 20:
                    sentences = verbatim.split('.')
                    for s in sentences[:2]:
                        if len(s) > 10:
                            sentiment_phrases['negativ'].append(self._anonymize_sentence(s))
                            
        return sentiment_phrases
    
    def _initialize_topics(self):
        """
        Initializes topics with exact weights from analysis of 17,884 feedbacks.
        CRITICAL: Tire change (only 5x in original!) and cleaning (0x) were added.
        
        Returns:
            None
        """
        
        # MASSIV ERWEITERT - Basierend auf Analyse von 17.884 echten Feedbacks
        # Gewichtung nach tatsÃ¤chlicher HÃ¤ufigkeit:
        # Service & Beratung: 30.6%, Terminvergabe: 30.1%, FahrzeugqualitÃ¤t: 15.6%
        self.topics_hierarchy = {
            'Service & Beratung': {  # 30.6% - HÃ–CHSTE PRIORITÃ„T
                'subtopics': [
                    'Freundlichkeit und HÃ¶flichkeit', 'Kompetenz der Berater', 
                    'LÃ¶sungsorientierung', 'Nachbetreuung', 'Kulanz',
                    'BeratungsqualitÃ¤t', 'Erreichbarkeit', 'Reaktionsgeschwindigkeit',
                    'Kundenorientierung', 'Beschwerdemanagement'
                ],
                'weight': 0.306
            },
            'Terminvergabe': {  # 30.1% - HÃ–CHSTE PRIORITÃ„T
                'subtopics': [
                    'VerfÃ¼gbarkeit von Terminen', 'FlexibilitÃ¤t bei Terminen',
                    'Online-Terminbuchung', 'Wartezeit auf Termin', 'PÃ¼nktlichkeit',
                    'Spontane Termine', 'Terminabsage', 'TerminbestÃ¤tigung',
                    'Terminverschiebung', 'Wartezeit vor Ort'
                ],
                'weight': 0.301
            },
            'FahrzeugqualitÃ¤t': {  # 15.6%
                'subtopics': [
                    'VerarbeitungsqualitÃ¤t', 'ZuverlÃ¤ssigkeit', 'Ausstattung',
                    'Design', 'Fahreigenschaften', 'MaterialqualitÃ¤t',
                    'LackqualitÃ¤t', 'Technische Features', 'Langlebigkeit'
                ],
                'weight': 0.156
            },
            'Kommunikation': {  # 9.2%
                'subtopics': [
                    'Transparenz bei Kosten', 'Erreichbarkeit telefonisch',
                    'RÃ¼ckmeldung nach Anfrage', 'VerstÃ¤ndlichkeit',
                    'Proaktive Information', 'E-Mail-Kommunikation',
                    'Status-Updates', 'Sprachbarrieren'
                ],
                'weight': 0.092
            },
            'Werkstattservice': {  # 5.6%
                'subtopics': [
                    'ReparaturqualitÃ¤t', 'Inspektion', 'Wartung', 'Diagnose',
                    'TeilequalitÃ¤t', 'Sauberkeit Werkstatt', 'Ausstattung Werkstatt',
                    'Wartebereich', 'Ersatzteilbeschaffung'
                ],
                'weight': 0.056
            },
            'Preis-Leistung': {  # 4.3%
                'subtopics': [
                    'Werkstattkosten', 'Ersatzteilpreise', 'Fairness der Preise',
                    'Kostenvoranschlag', 'Zusatzkosten', 'Preistransparenz',
                    'Garantieleistungen', 'Kulanzregelung'
                ],
                'weight': 0.043
            },
            'Mitarbeiter': {  # 2.6%
                'subtopics': [
                    'Serviceberater', 'Mechaniker', 'Empfangspersonal',
                    'Verkaufspersonal', 'Management', 'Teamwork',
                    'ProfessionalitÃ¤t', 'Erscheinungsbild'
                ],
                'weight': 0.026
            },
            # UNTERREPRÃ„SENTIERT - Gezielt hinzugefÃ¼gt (nur 5 Vorkommen in Originaldaten!)
            'Reifenwechsel': {  # 1.5% - MUSS GENERIERT WERDEN
                'subtopics': [
                    'Sommer-/Winterreifen Wechsel', 'TerminverfÃ¼gbarkeit Reifenwechsel',
                    'Preis fÃ¼r Reifenwechsel', 'Einlagerung der Reifen',
                    'Reifenzustand prÃ¼fen', 'Auswuchten der Reifen',
                    'Ventile ersetzen', 'Dauer des Reifenwechsels',
                    'Online-Buchung Reifenwechsel', 'Express-Reifenwechsel',
                    'Reifenkauf beim Wechsel', 'Reifenentsorgung',
                    'Wartezeit Reifenwechsel', 'Rdks-System Anpassung',
                    'Felgenreinigung beim Wechsel'
                ],
                'weight': 0.015
            },
            # KOMPLETT FEHLEND - Gezielt ergÃ¤nzt (0 Vorkommen in Originaldaten!)
            'Reinigung & Pflege': {  # 1.0% - NEU GENERIEREN
                'subtopics': [
                    'FahrzeugwÃ¤sche nach Service', 'Innenraumreinigung',
                    'Aufbereitung des Fahrzeugs', 'Sauberhaltung bei Reparatur',
                    'Polsterreinigung', 'Lack aufpolieren',
                    'MotorwÃ¤sche', 'Felgenreinigung',
                    'Scheibenreinigung innen', 'Geruchsbeseitigung',
                    'Lederpflege', 'Kunststoffpflege',
                    'UnterbodenwÃ¤sche', 'Steinschlag-Behandlung',
                    'Nano-Versiegelung'
                ],
                'weight': 0.010
            },
            'Ersatzfahrzeug': {  # 0.8%
                'subtopics': [
                    'VerfÃ¼gbarkeit Ersatzwagen', 'Zustand Ersatzfahrzeug',
                    'Kategorie Leihwagen', 'Buchung Ersatzwagen',
                    'Kosten Leihfahrzeug', 'Versicherung Ersatzwagen',
                    'Tankregelung Leihwagen'
                ],
                'weight': 0.008
            },
            'Digitale Services': {  # 0.5%
                'subtopics': [
                    'App-Nutzung', 'Website-FunktionalitÃ¤t', 'Online-Portal',
                    'Digitale Rechnung', 'SMS/E-Mail Updates',
                    'Online-Terminbuchung', 'Digitaler Kostenvoranschlag'
                ],
                'weight': 0.005
            }
        }
        
        # Sentiments fÃ¼r Feedback-Generierung
        self.sentiments = ['positiv', 'neutral', 'negativ']
        
    def _initialize_fake_entities(self):
        """
        Initializes creative fake names for anonymization.
        
        Returns:
            None
        """
        
        # Kreative Werkstatt-Namen (lustig aber professionell)
        if self.enable_fun_mode:
            self.fake_dealerships = [
                "Autohaus Sonnenschein", "Werkstatt Blitzblank", "AutoCenter Regenbogen",
                "Motorwelt Sternschnuppe", "Autohaus GlÃ¼cksklee", "Service-Center Traumwagen",
                "Autopark Wunderland", "Werkstatt Meisterhaft", "AutoPalast KÃ¶nig",
                "Fahrzeugwelt Paradies", "Autohaus Goldgrube", "Service-Oase WÃ¼stenfuchs",
                "Motorhof Edelstein", "Autohaus Zeitreise", "Werkstatt Turbozauber",
                "AutoArena Champion", "Servicewelt Premiumglanz", "Autohaus Meilenstein",
                "Werkstatt SchraubenkÃ¶nig", "Motorreich Vollgas", "Autohaus Freudensprung",
                "Service-Station Rakete", "Autowelt Horizont", "Werkstatt PrÃ¤zision Plus",
                "Autohaus Vertrauenssache"
            ]
        else:
            self.fake_dealerships = [
                "Autohaus MÃ¼ller", "Werkstatt Schmidt", "AutoCenter Weber",
                "Motorwelt Fischer", "Autohaus Wagner", "Service-Center Becker",
                "Autopark Schulz", "Werkstatt Hoffmann", "AutoPalast KÃ¶nig"
            ]
        
        # Mitarbeiter-Namen (divers und anonym)
        self.fake_employee_names = {
            'male': [
                "Herr MÃ¼ller", "Herr Schmidt", "Herr Weber", "Herr Fischer", "Herr Meyer",
                "Herr Wagner", "Herr Becker", "Herr Schulz", "Herr Hoffmann", "Herr SchÃ¤fer",
                "Herr Koch", "Herr Bauer", "Herr Richter", "Herr Klein", "Herr Wolf"
            ],
            'female': [
                "Frau Schneider", "Frau Neumann", "Frau Schwarz", "Frau Zimmermann", "Frau Braun",
                "Frau KrÃ¼ger", "Frau Hofmann", "Frau Schmitt", "Frau Lange", "Frau Werner",
                "Frau Schmitz", "Frau Krause", "Frau Meier", "Frau Lehmann", "Frau KÃ¶hler"
            ],
            'neutral': [
                "Das Service-Team", "Die Werkstatt-Crew", "Das Beratungsteam", "Die Technikabteilung",
                "Der Kundenservice", "Das Empfangsteam", "Die Serviceberater", "Das Management"
            ]
        }
        
        # Fake StÃ¤dte/Regionen (kreativ aber realistisch)
        self.fake_cities = [
            "Neustadt", "Altdorf", "Bergheim", "Seestadt", "Waldburg", "Sonnenberg",
            "Rosenheim", "GrÃ¼nwald", "Steinbach", "Goldbach", "Silbertal", "Kupferberg",
            "Eisenstadt", "Blaubeuren", "Rotenburg", "WeiÃŸkirchen", "Schwarzwald",
            "Friedrichshafen", "Wilhelmsburg", "Karlsfeld", "Ludwigshafen", "Marienberg"
        ]
        
        # Anonyme Kunden-IDs
        self.customer_id_prefixes = ["CUST", "KND", "USR", "CLT", "FDB"]
        
    def _initialize_personas(self):
        """
        Defines diverse customer personas for realistic variation.
        
        Returns:
            None
        """
        
        self.personas = {
            'digital_native': PersonaProfile(
                age_group='18-35',
                gender='mixed',
                education_level='high',
                tech_affinity='very_high',
                communication_style='casual',
                typical_concerns=['App', 'Online-Services', 'Geschwindigkeit', 'Transparenz'],
                text_patterns={'short': 0.6, 'medium': 0.3, 'long': 0.1},
                typo_probability=0.15,
                emoji_usage=0.2,
                formality_level=0.3
            ),
            'busy_professional': PersonaProfile(
                age_group='30-50',
                gender='mixed',
                education_level='high',
                tech_affinity='high',
                communication_style='efficient',
                typical_concerns=['Zeit', 'FlexibilitÃ¤t', 'QualitÃ¤t', 'VerlÃ¤sslichkeit'],
                text_patterns={'short': 0.3, 'medium': 0.5, 'long': 0.2},
                typo_probability=0.05,
                emoji_usage=0.02,
                formality_level=0.7
            ),
            'experienced_senior': PersonaProfile(
                age_group='60+',
                gender='mixed',
                education_level='mixed',
                tech_affinity='low',
                communication_style='formal',
                typical_concerns=['Beratung', 'Vertrauen', 'Service', 'Preis'],
                text_patterns={'short': 0.1, 'medium': 0.4, 'long': 0.5},
                typo_probability=0.08,
                emoji_usage=0.01,
                formality_level=0.9
            ),
            'family_oriented': PersonaProfile(
                age_group='35-50',
                gender='mixed',
                education_level='mixed',
                tech_affinity='medium',
                communication_style='friendly',
                typical_concerns=['Sicherheit', 'Preis-Leistung', 'Kinderfreundlichkeit', 'ZuverlÃ¤ssigkeit'],
                text_patterns={'short': 0.2, 'medium': 0.6, 'long': 0.2},
                typo_probability=0.1,
                emoji_usage=0.05,
                formality_level=0.5
            ),
            'tech_enthusiast': PersonaProfile(
                age_group='25-45',
                gender='mixed',
                education_level='high',
                tech_affinity='very_high',
                communication_style='technical',
                typical_concerns=['Innovation', 'Features', 'Performance', 'Updates'],
                text_patterns={'short': 0.2, 'medium': 0.5, 'long': 0.3},
                typo_probability=0.03,
                emoji_usage=0.1,
                formality_level=0.6
            ),
            'price_conscious': PersonaProfile(
                age_group='mixed',
                gender='mixed',
                education_level='mixed',
                tech_affinity='medium',
                communication_style='direct',
                typical_concerns=['Kosten', 'Angebote', 'Rabatte', 'Transparenz'],
                text_patterns={'short': 0.4, 'medium': 0.5, 'long': 0.1},
                typo_probability=0.12,
                emoji_usage=0.03,
                formality_level=0.4
            )
        }
        
    def _initialize_markets_and_regions(self):
        """
        Initializes markets in correct format: Region-Country.
        Example: C1-DE, C1-CH, C3-CN, C5-US
        
        Regions: C1 (Europe), C3 (Asia), C5 (North America)
        
        Returns:
            None
        """
        
        # Korrekte Market-Struktur mit Region-Country Format
        self.markets = [
            # C1 - Europa
            'C1-DE',  # Deutschland
            'C1-CH',  # Schweiz
            'C1-IT',  # Italien
            'C1-FR',  # Frankreich
            
            # C3 - Asien
            'C3-CN',  # China
            'C3-KR',  # Korea
            'C3-JP',  # Japan
            
            # C5 - Nordamerika
            'C5-US',  # USA
            'C5-CA',  # Kanada
        ]
        
        # Mapping: Market -> Region & Country
        self.market_mapping = {}
        for market in self.markets:
            parts = market.split('-')
            region = parts[0]
            country = parts[1]
            self.market_mapping[market] = {
                'region': region,
                'country': country
            }
        
        # Regionen-Info
        self.regions = {
            'C1': 'Europa',
            'C3': 'Asien',
            'C5': 'Nordamerika'
        }
        
    def _initialize_text_components(self):
        """
        Initializes text components based on REAL customer feedbacks.
        
        Returns:
            None
        """
        
        # MASSIV ERWEITERT - 100+ Variationen statt monotoner "customer states" Phrasen
        self.text_components = {
            'openings': {
                'positiv': [
                    # Direkte ZufriedenheitsÃ¤uÃŸerungen (20%)
                    "Ich bin sehr zufrieden mit",
                    "Ausgezeichneter Service bei", 
                    "Der Service war einwandfrei bei",
                    "Bin rundum zufrieden mit",
                    "Top Service und kaum Wartezeit bei",
                    "Hervorragend war die Betreuung bei",
                    "Sehr positiv Ã¼berrascht von",
                    "Kann ich nur empfehlen:",
                    "Perfekt gelaufen ist mein Termin bei",
                    "Absolut zufrieden war ich mit",
                    "Begeistert bin ich von",
                    "AuÃŸergewÃ¶hnlich gut war der Service bei",
                    "Herausragend fand ich",
                    "Ãœberzeugt hat mich",
                    "Fantastische Erfahrung bei",
                    
                    # Lobende Aussagen (20%)
                    "Der Kunde lobt ausdrÃ¼cklich",
                    "Positiv hervorgehoben wird",
                    "Besonders gefallen hat dem Kunden",
                    "Der Kunde betont die exzellente QualitÃ¤t bei",
                    "Zufrieden zeigt sich der Kunde mit",
                    "Der Kunde wÃ¼rde weiterempfehlen:",
                    "Beeindruckt war der Kunde von",
                    "Der Kunde hebt positiv hervor:",
                    "Sehr angetan ist der Kunde von",
                    
                    # Situative Positive (20%)
                    "Bei meinem letzten Besuch bei",
                    "Mein Termin verlief hervorragend bei",
                    "Die Inspektion war bestens organisiert bei",
                    "Alles lief reibungslos bei",
                    "Problemlos und schnell ging es bei",
                    "Ãœberraschend positiv verlief mein Besuch bei",
                    "Sehr angenehm war mein Aufenthalt bei",
                    
                    # PersÃ¶nliche Empfehlungen (20%)
                    "Ich empfehle definitiv",
                    "Ohne EinschrÃ¤nkung kann ich empfehlen:",
                    "Gerne wieder bei",
                    "Jederzeit wieder zu",
                    "Absolut empfehlenswert ist",
                    "Meine erste Wahl ist",
                    "Von mir 5 Sterne fÃ¼r",
                    
                    # Emotionale Positive (20%)
                    "Ich war begeistert von",
                    "Es freut mich sehr, dass",
                    "Ich bin froh, dass ich mich fÃ¼r",
                    "Sehr dankbar bin ich fÃ¼r",
                    "Toll war es bei"
                ],
                'neutral': [
                    # Sachliche Beschreibungen (40%)
                    "War heute bei",
                    "Hatte einen Termin bei",
                    "War zur Inspektion bei",
                    "Habe mein Fahrzeug gebracht zu",
                    "Meine Erfahrung mit",
                    "War wegen Reifenwechsel bei",
                    "Hatte einen Service-Termin bei",
                    "War zur Wartung bei",
                    "Der Termin fand statt bei",
                    "Gestern war ich bei",
                    "Diese Woche hatte ich einen Termin bei",
                    "Am Montag besuchte ich",
                    "Vergangene Woche war ich bei",
                    "KÃ¼rzlich hatte ich einen Termin bei",
                    "Letzte Woche brachte ich mein Fahrzeug zu",
                    
                    # Berichtende Aussagen (30%)
                    "Der Kunde berichtet von seinem Besuch bei",
                    "Laut Aussage des Kunden",
                    "Der Kunde schildert seine Erfahrung mit",
                    "Im Rahmen der Inspektion bei",
                    "Der Kunde beschreibt seinen Termin bei",
                    "Folgende RÃ¼ckmeldung kam vom Kunden:",
                    "Der Kunde teilt seine EindrÃ¼cke von",
                    "Die Kundin berichtet Ã¼ber",
                    
                    # Neutrale Feststellungen (30%)
                    "Mein Eindruck von",
                    "Folgendes ist mir aufgefallen bei",
                    "Im GroÃŸen und Ganzen war mein Termin bei",
                    "GrundsÃ¤tzlich verlief der Besuch bei",
                    "Soweit ich das beurteilen kann, war",
                    "Meine Wahrnehmung bei",
                    "Was mir bei diesem Termin auffiel:"
                ],
                'negativ': [
                    # EnttÃ¤uschungsÃ¤uÃŸerungen (25%)
                    "Leider sehr enttÃ¤uschend war mein Termin bei",
                    "GroÃŸe Probleme gab es bei",
                    "Absolut nicht zufrieden mit",
                    "Katastrophaler Service bei",
                    "Sehr schlechte Erfahrung mit",
                    "Unzumutbar war der Service bei",
                    "Bodenlose Frechheit bei",
                    "Nie wieder",
                    "Stark enttÃ¤uscht bin ich von",
                    "Leider muss ich negativ berichten Ã¼ber",
                    "Sehr Ã¤rgerlich war mein Besuch bei",
                    "Frustrierend verlief mein Termin bei",
                    
                    # Kritische Aussagen (25%)
                    "Der Kunde kritisiert deutlich",
                    "Unzufrieden Ã¤uÃŸert sich der Kunde Ã¼ber",
                    "Der Kunde bemÃ¤ngelt",
                    "Kritisch bewertet der Kunde",
                    "Der Kunde beschwert sich Ã¼ber",
                    "Negativ aufgefallen ist dem Kunden bei",
                    "Der Kunde ist verÃ¤rgert Ã¼ber",
                    "EnttÃ¤uscht zeigt sich der Kunde von",
                    
                    # Konkrete Problembeschreibungen (25%)
                    "Folgende Probleme traten auf bei",
                    "Leider verlief nicht alles wie geplant bei",
                    "Mehrere MÃ¤ngel fielen auf bei",
                    "Nicht zufriedenstellend war bei",
                    "Es gab erhebliche Schwierigkeiten bei",
                    "Ã„rgerlich war die Situation bei",
                    
                    # Direkte Unzufriedenheit (25%)
                    "Ich bin sehr unzufrieden mit",
                    "Das war inakzeptabel bei",
                    "So geht man nicht mit Kunden um:",
                    "Eine Zumutung war der Besuch bei",
                    "Absolut nicht zu empfehlen:",
                    "Ich werde nicht mehr hingehen zu",
                    "Das war mein letzter Besuch bei",
                    "Unter aller Kanone war"
                ]
            },
            
            # AUS ECHTEN DATEN: HÃ¤ufige Probleme
            'real_complaints': {
                'wartezeit': [
                    "musste Ã¼ber 2 Stunden warten obwohl Termin vereinbart war",
                    "Wartezeit war viel lÃ¤nger als angekÃ¼ndigt",
                    "statt der versprochenen 30 Minuten wurden es 2 Stunden",
                    "musste 3 Stunden warten fÃ¼r einen einfachen Reifenwechsel",
                    "trotz Termin um 9 Uhr wurde mein Fahrzeug erst um 11 Uhr angenommen",
                    "die angegebene Wartezeit von einer Stunde wurde deutlich Ã¼berschritten",
                    "wartete Ã¼ber 1,5 Stunden obwohl es hieÃŸ maximal 45 Minuten"
                ],
                'kommunikation': [
                    "wurde nicht zurÃ¼ckgerufen wie versprochen",
                    "niemand konnte mir Auskunft geben",
                    "mehrmals angerufen aber nie jemanden erreicht",
                    "die versprochene RÃ¼ckmeldung kam nie",
                    "musste selbst mehrfach nachfragen",
                    "Informationen waren widersprÃ¼chlich",
                    "wurde am Telefon falsch informiert"
                ],
                'service': [
                    "Mitarbeiter war sehr unfreundlich",
                    "fÃ¼hlte mich nicht willkommen",
                    "wurde von oben herab behandelt",
                    "Personal wirkte genervt und desinteressiert",
                    "Empfang war sehr unfreundlich",
                    "Service-Berater hatte keine Zeit fÃ¼r mich",
                    "wurde einfach stehen gelassen"
                ],
                'werkstatt': [
                    "Problem wurde nicht behoben",
                    "zusÃ¤tzliche Reparaturen ohne RÃ¼cksprache durchgefÃ¼hrt",
                    "Fahrzeug war nicht fertig wie versprochen",
                    "Rechnung war viel hÃ¶her als Kostenvoranschlag",
                    "Arbeiten wurden nicht korrekt ausgefÃ¼hrt",
                    "musste nochmal kommen weil Fehler gemacht wurde",
                    "versprochene FahrzeugwÃ¤sche wurde vergessen"
                ],
                'kosten': [
                    "Rechnung war doppelt so hoch wie angegeben",
                    "versteckte Kosten die vorher nicht genannt wurden",
                    "musste fÃ¼r Leistungen zahlen die nicht bestellt waren",
                    "Preise sind eine absolute Frechheit",
                    "fÃ¼r 5 Liter Ã–l 200 Euro ist Wucher",
                    "Stundensatz von 200 Euro ist unverschÃ¤mt"
                ]
            },
            
            # AUS ECHTEN DATEN: Positive Aspekte
            'real_praise': {
                'service': [
                    "Mitarbeiter war sehr freundlich und kompetent",
                    "wurde sehr gut beraten",
                    "Service-Berater nahm sich viel Zeit",
                    "fÃ¼hlte mich gut aufgehoben",
                    "Personal war Ã¤uÃŸerst hilfsbereit",
                    "kompetente und ehrliche Beratung"
                ],
                'werkstatt': [
                    "Arbeit wurde sauber ausgefÃ¼hrt",
                    "Problem wurde schnell gefunden und behoben",
                    "Fahrzeug war pÃ¼nktlich fertig",
                    "faire Preise und transparente Abrechnung",
                    "Kostenvoranschlag wurde eingehalten"
                ],
                'wartezeit': [
                    "kurze Wartezeit",
                    "ging schneller als erwartet",
                    "konnte direkt warten",
                    "war in 30 Minuten fertig",
                    "keine Wartezeit",
                    "super schnell"
                ]
            },
            
            # ECHTE SchlusssÃ¤tze aus Ihren Daten
            'closings': {
                'positiv': [
                    "Weiter so!",
                    "Immer wieder gerne!",
                    "Kann ich nur empfehlen.",
                    "Top Service!",
                    "Bin sehr zufrieden.",
                    "5 Sterne!",
                    "Gerne wieder!",
                    "Absolut empfehlenswert!"
                ],
                'neutral': [
                    "War okay.",
                    "Geht so.",
                    "Durchschnittlich.",
                    "Standard.",
                    "In Ordnung.",
                    "Akzeptabel."
                ],
                'negativ': [
                    "Sehr enttÃ¤uschend.",
                    "Nicht empfehlenswert.",
                    "Werde die Werkstatt wechseln.",
                    "Nie wieder!",
                    "Katastrophe!",
                    "Absolut inakzeptabel!",
                    "FÃ¼r mich war es das letzte Mal.",
                    "Werde zu einem anderen HÃ¤ndler gehen."
                ]
            }
        }
        
        # Spezifische Beschwerden/Lob nach Thema
        self.specific_feedback_elements = {
            'Werkstatt': {
                'positiv': [
                    "schnelle Diagnose", "kompetente Mechaniker", "faire Preise",
                    "saubere Arbeit", "termingerechte Fertigstellung", "gute Beratung",
                    "transparente Kostenaufstellung", "professionelle DurchfÃ¼hrung"
                ],
                'negativ': [
                    "lange Wartezeiten", "ungenaue Diagnose", "Ã¼berhÃ¶hte Preise",
                    "unsaubere Arbeit", "Termine nicht eingehalten", "schlechte Kommunikation",
                    "versteckte Kosten", "inkompetente Mitarbeiter"
                ]
            },
            'Service': {
                'positiv': [
                    "freundliche Mitarbeiter", "kompetente Beratung", "schnelle Hilfe",
                    "individuelle LÃ¶sungen", "zuvorkommend", "hilfsbereit",
                    "professionell", "kundenorientiert"
                ],
                'negativ': [
                    "unfreundliches Personal", "keine Beratung", "lange Wartezeiten",
                    "desinteressiert", "unhÃ¶flich", "inkompetent",
                    "nicht erreichbar", "schlechte Einstellung"
                ]
            }
        }
        
    def _initialize_temporal_patterns(self):
        """
        Defines temporal patterns for more realistic data.
        
        Returns:
            None
        """
        
        self.temporal_patterns = {
            'seasonal': {
                'winter': {'Werkstatt': 0.3, 'Ersatzfahrzeug': 0.2},  # Mehr Werkstattbesuche
                'summer': {'Terminvergabe': -0.1, 'Service': 0.1},   # Urlaubszeit
                'spring': {'FahrzeugqualitÃ¤t': 0.2},                  # NeuwagenkÃ¤ufe
                'autumn': {'Preis-Leistung': 0.15}                    # Jahresend-Angebote
            },
            'weekday': {
                0: -0.1,  # Montag - negativer
                1: 0.0,   # Dienstag
                2: 0.0,   # Mittwoch
                3: 0.05,  # Donnerstag
                4: 0.1,   # Freitag - positiver
                5: 0.05,  # Samstag
                6: -0.05  # Sonntag
            },
            'time_of_day': {
                'morning': {'efficiency': 0.1, 'friendliness': -0.05},
                'noon': {'efficiency': 0.0, 'friendliness': 0.1},
                'afternoon': {'efficiency': -0.05, 'friendliness': 0.05},
                'evening': {'efficiency': -0.1, 'friendliness': -0.1}
            }
        }
        
    def _generate_customer_id(self) -> str:
        """Generiert eine anonyme Kunden-ID"""
        prefix = random.choice(self.customer_id_prefixes)
        numbers = ''.join(random.choices(string.digits, k=6))
        suffix = ''.join(random.choices(string.ascii_uppercase, k=2))
        return f"{prefix}-{numbers}-{suffix}"
        
    def _generate_session_id(self) -> str:
        """
        Generates a session ID for tracking.
        
        Returns:
            str: Session ID in format SID-TIMESTAMP-HEXDIGITS
        """
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        random_part = ''.join(random.choices(string.hexdigits, k=8))
        return f"SID-{timestamp}-{random_part}"
        
    def _select_persona(self) -> Tuple[str, PersonaProfile]:
        """
        Selects a persona with weighting for diversity.
        
        Returns:
            Tuple[str, PersonaProfile]: Tuple of (persona_name, persona_profile)
        """
        # Gewichtete Auswahl fÃ¼r realistische Verteilung
        weights = {
            'digital_native': 0.25,
            'busy_professional': 0.20,
            'experienced_senior': 0.15,
            'family_oriented': 0.20,
            'tech_enthusiast': 0.10,
            'price_conscious': 0.10
        }
        
        persona_name = random.choices(
            list(weights.keys()),
            weights=list(weights.values())
        )[0]
        
        return persona_name, self.personas[persona_name]
        
    def _apply_persona_style(self, text: str, persona: PersonaProfile) -> str:
        """
        Applies persona-specific language patterns.
        
        Args:
            text (str): Text to apply persona style to
            persona (PersonaProfile): Persona profile with style preferences
            
        Returns:
            str: Text with applied persona style
        """
        
        # Formality anpassen
        if persona.formality_level < 0.3:
            # Casual style
            replacements = [
                ("Sehr geehrte", "Hey"),
                ("Mit freundlichen GrÃ¼ÃŸen", "LG"),
                ("Ich mÃ¶chte", "Ich will"),
                ("kÃ¶nnten Sie", "kÃ¶nnt ihr")
            ]
            for old, new in replacements:
                text = text.replace(old, new)
                
        elif persona.formality_level > 0.7:
            # Formal style - DIVERSIFIZIERT statt monoton
            if random.random() < 0.15:  # Nur 15% statt immer
                formal_openings = [
                    "Sehr geehrte Damen und Herren, ",
                    "Guten Tag, ",
                    "Hallo, ",
                    "",  # Kein Opening
                ]
                text = random.choice(formal_openings) + text
            if not text.endswith("."):
                text += "."
            # Closings auch variieren
            if random.random() < 0.15:  # Nur 15%
                formal_closings = [
                    " Mit freundlichen GrÃ¼ÃŸen",
                    " Vielen Dank",
                    " Beste GrÃ¼ÃŸe",
                    ""  # Kein Closing
                ]
                text += random.choice(formal_closings)
            
        # Emojis hinzufÃ¼gen
        if random.random() < persona.emoji_usage:
            emoji_map = {
                'positiv': ['ðŸ˜Š', 'ðŸ‘', 'â­', 'âœ…', 'ðŸŽ‰', 'ðŸ’¯'],
                'neutral': ['ðŸ¤”', 'ðŸ˜', 'ðŸ¤·', 'ðŸ“', 'âž¡ï¸'],
                'negativ': ['ðŸ˜ž', 'ðŸ‘Ž', 'ðŸ˜ ', 'âŒ', 'ðŸ˜¤', 'ðŸ’”']
            }
            sentiment = self._detect_sentiment(text)
            emoji = random.choice(emoji_map.get(sentiment, ['']))
            text = text + " " + emoji
            
        # Tippfehler hinzufÃ¼gen
        if random.random() < persona.typo_probability:
            text = self._add_realistic_typos(text)
            
        return text
        
    def _add_realistic_typos(self, text: str) -> str:
        """
        Adds realistic typos to text.
        
        Args:
            text (str): Text to add typos to
            
        Returns:
            str: Text with realistic typos added
        """
        typo_types = [
            self._swap_adjacent_chars,
            self._duplicate_char,
            self._missing_char,
            self._wrong_case,
            self._common_misspelling
        ]
        
        # 1-3 Fehler pro Text (aber mindestens 10 WÃ¶rter nÃ¶tig)
        words = len(text.split())
        if words < 10:
            return text  # Zu kurz fÃ¼r Typos
            
        num_typos = random.randint(1, min(3, words // 10))
        
        for _ in range(num_typos):
            typo_func = random.choice(typo_types)
            text = typo_func(text)
            
        return text
        
    def _swap_adjacent_chars(self, text: str) -> str:
        """
        Swaps adjacent characters.
        
        Args:
            text (str): Text to modify
            
        Returns:
            str: Text with swapped characters
        """
        words = text.split()
        if words:
            word_idx = random.randint(0, len(words) - 1)
            word = words[word_idx]
            if len(word) > 2:
                char_idx = random.randint(0, len(word) - 2)
                word = word[:char_idx] + word[char_idx+1] + word[char_idx] + word[char_idx+2:]
                words[word_idx] = word
        return ' '.join(words)
        
    def _duplicate_char(self, text: str) -> str:
        """
        Duplicates a character.
        
        Args:
            text (str): Text to modify
            
        Returns:
            str: Text with duplicated character
        """
        words = text.split()
        if words:
            word_idx = random.randint(0, len(words) - 1)
            word = words[word_idx]
            if len(word) > 1:
                char_idx = random.randint(0, len(word) - 1)
                word = word[:char_idx] + word[char_idx] + word[char_idx:]
                words[word_idx] = word
        return ' '.join(words)
        
    def _missing_char(self, text: str) -> str:
        """
        Removes a character.
        
        Args:
            text (str): Text to modify
            
        Returns:
            str: Text with missing character
        """
        words = text.split()
        if words:
            word_idx = random.randint(0, len(words) - 1)
            word = words[word_idx]
            if len(word) > 2:
                char_idx = random.randint(1, len(word) - 1)
                word = word[:char_idx] + word[char_idx+1:]
                words[word_idx] = word
        return ' '.join(words)
        
    def _wrong_case(self, text: str) -> str:
        """
        Changes capitalization.
        
        Args:
            text (str): Text to modify
            
        Returns:
            str: Text with changed capitalization
        """
        words = text.split()
        if words:
            word_idx = random.randint(0, len(words) - 1)
            words[word_idx] = words[word_idx].lower() if word_idx == 0 else words[word_idx].upper()
        return ' '.join(words)
        
    def _common_misspelling(self, text: str) -> str:
        """
        Adds common misspellings.
        
        Args:
            text (str): Text to modify
            
        Returns:
            str: Text with common misspellings
        """
        common_errors = [
            ("das", "dass"), ("dass", "das"),
            ("seit", "seid"), ("seid", "seit"),
            ("wieder", "wider"), ("wider", "wieder"),
            ("ihr", "Ihr"), ("Ihr", "ihr")
        ]
        for old, new in common_errors:
            if old in text and random.random() < 0.3:
                text = text.replace(old, new, 1)
                break
        return text
        
    def _generate_dynamic_feedback_text(
        self, 
        topic: str, 
        sentiment: str, 
        persona: PersonaProfile,
        subtopic: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> str:
        """
        Generates dynamic feedback text based on learned patterns with REALISTIC TEXT LENGTHS.
        
        Args:
            topic (str): Main topic of the feedback
            sentiment (str): Sentiment type ('positiv', 'neutral', 'negativ')
            persona (PersonaProfile): Persona profile for style application
            subtopic (Optional[str]): Specific subtopic. Defaults to None
            context (Optional[Dict]): Additional context information. Defaults to None
            
        Returns:
            str: Generated feedback text
        """
        
        # Berechne Ziel-Wortanzahl basierend auf Sentiment
        target_length = self.text_length_controller.get_realistic_length(sentiment)
        
        # Wenn wir aus echten Daten gelernt haben, nutze diese Muster
        if self.learned_patterns and random.random() < 0.7:  # 70% basiert auf echten Daten
            text = self._generate_from_learned_patterns(topic, sentiment, persona)
        else:
            # Basis-Struktur aufbauen
            components = []
            
            # Opening
            opening = random.choice(self.text_components['openings'][sentiment])
            
            # Werkstatt-Name einfÃ¼gen
            dealership = random.choice(self.fake_dealerships)
            opening = opening + f" {dealership}"
            components.append(opening)
            
            # Hauptteil - ERWEITERT fÃ¼r realistische LÃ¤ngen
            main_points = []
            
            # Verwende echte Phrasen aus den Daten
            if sentiment == 'negativ' and 'real_complaints' in self.text_components:
                # WÃ¤hle eine Kategorie von Beschwerden
                complaint_category = random.choice(list(self.text_components['real_complaints'].keys()))
                complaint = random.choice(self.text_components['real_complaints'][complaint_category])
                main_points.append(complaint)
                
            elif sentiment == 'positiv' and 'real_praise' in self.text_components:
                # WÃ¤hle Lob-Kategorie
                praise_category = random.choice(list(self.text_components['real_praise'].keys()))
                praise = random.choice(self.text_components['real_praise'][praise_category])
                main_points.append(praise)
                
            # FÃ¼ge weitere Details hinzu um ZiellÃ¤nge zu erreichen
            current_length = len(' '.join(components + main_points).split())
            
            # Detaillierte Elemente fÃ¼r lÃ¤ngere Texte
            while current_length < target_length * 0.8:  # Mindestens 80% der ZiellÃ¤nge
                detail_options = []
                
                # Mitarbeiter-ErwÃ¤hnung
                if random.random() < 0.5:
                    gender = random.choice(['male', 'female', 'neutral'])
                    employee = random.choice(self.fake_employee_names[gender])
                    if sentiment == 'positiv':
                        details = [
                            f"{employee} war sehr kompetent und freundlich",
                            f"Besonders {employee} hat sich sehr bemÃ¼ht",
                            f"Die Beratung durch {employee} war erstklassig"
                        ]
                    elif sentiment == 'negativ':
                        details = [
                            f"{employee} war leider nicht hilfreich",
                            f"Von {employee} fÃ¼hlte ich mich nicht ernst genommen",
                            f"Die Kommunikation mit {employee} verlief enttÃ¤uschend"
                        ]
                    else:
                        details = [
                            f"{employee} hat mich betreut",
                            f"Mein Ansprechpartner war {employee}"
                        ]
                    detail_options.extend(details)
                
                # Zeitangaben
                if random.random() < 0.4:
                    detail_options.extend([
                        f"Die Wartezeit betrug etwa {random.randint(15, 180)} Minuten",
                        f"Ich hatte meinen Termin um {random.randint(8, 17)}:00 Uhr",
                        f"Der gesamte Vorgang dauerte {random.choice(['lÃ¤nger als erwartet', 'wie versprochen', 'Ã¼berraschend kurz'])}"
                    ])
                
                # Topic-spezifische Details
                if topic and random.random() < 0.6:
                    detail_options.extend([
                        f"Es ging um {topic.lower()}",
                        f"BezÃ¼glich {topic.lower()} hatte ich einige Fragen",
                        f"Der {topic.lower()}-Service wurde durchgefÃ¼hrt"
                    ])
                
                # WÃ¤hle ein Detail
                if detail_options:
                    main_points.append(random.choice(detail_options))
                    current_length = len(' '.join(components + main_points).split())
                else:
                    break  # Keine weiteren Details verfÃ¼gbar
            
            # Mitarbeiter erwÃ¤hnen (falls noch nicht geschehen)
            if random.random() < 0.3 and current_length < target_length:
                gender = random.choice(['male', 'female', 'neutral'])
                employee = random.choice(self.fake_employee_names[gender])
                if sentiment == 'positiv':
                    main_points.append(f"{employee} war sehr kompetent")
                elif sentiment == 'negativ':
                    main_points.append(f"{employee} war leider nicht hilfreich")
                    
            # ZusammenfÃ¼gen
            if main_points:
                main_text = ". ".join(main_points)
            else:
                main_text = ""
                
            # Closing
            closing = random.choice(self.text_components['closings'][sentiment])
            
            # VollstÃ¤ndiger Text
            text = opening + ". " + main_text + ". " + closing
        
        # Persona-Style anwenden (NACH LÃ¤ngen-Anpassung)
        text = self._apply_persona_style(text, persona)
        
        return text.strip()
    
    def _generate_from_learned_patterns(
        self, 
        topic: str, 
        sentiment: str, 
        persona: PersonaProfile
    ) -> str:
        """
        Generates text based on learned patterns from real data.
        
        Args:
            topic (str): Main topic of the feedback
            sentiment (str): Sentiment type ('positiv', 'neutral', 'negativ')
            persona (PersonaProfile): Persona profile for style application
            
        Returns:
            str: Generated feedback text based on learned patterns
        """
        
        # WÃ¤hle eine Werkstatt
        dealership = random.choice(self.fake_dealerships)
        
        # Basis-Opening
        opening = f"Mein Termin bei {dealership}"
        
        # Hauptteil aus gelernten Mustern
        main_parts = []
        
        if sentiment == 'negativ':
            # Nutze gelernte Probleme
            if 'phrase_patterns' in self.learned_patterns:
                categories = ['wartezeit', 'service', 'kommunikation', 'werkstatt', 'kosten']
                chosen_categories = random.sample(categories, min(2, len(categories)))
                
                for category in chosen_categories:
                    if category in self.learned_patterns['phrase_patterns']:
                        patterns = self.learned_patterns['phrase_patterns'][category]
                        if patterns:
                            phrase = random.choice(patterns)
                            # Ersetze Platzhalter mit Fake-Daten
                            phrase = phrase.replace('XX:XX', f"{random.randint(8,18)}:{random.choice(['00','15','30','45'])}")
                            phrase = phrase.replace('XXXXX', str(random.randint(100, 2000)))
                            main_parts.append(phrase)
                            
            # FÃ¼ge hÃ¤ufige Probleme hinzu
            if 'common_issues' in self.learned_patterns and self.learned_patterns['common_issues']:
                issue = random.choice(self.learned_patterns['common_issues'])
                main_parts.append(f"Das Hauptproblem war: {issue}")
                
        elif sentiment == 'positiv':
            # Positive Aspekte
            positive_aspects = [
                "Der Service war ausgezeichnet",
                "Sehr kompetente Beratung",
                "Faire Preise",
                "Schnelle Abwicklung",
                "Freundliches Personal"
            ]
            main_parts.extend(random.sample(positive_aspects, 2))
            
        # Text zusammensetzen
        if main_parts:
            main_text = ". ".join(main_parts)
        else:
            main_text = "war durchschnittlich"
            
        # Abschluss basierend auf Sentiment
        if sentiment == 'negativ':
            closing = random.choice([
                "Werde die Werkstatt wechseln",
                "Sehr enttÃ¤uschend",
                "Nicht empfehlenswert",
                "Das war das letzte Mal"
            ])
        elif sentiment == 'positiv':
            closing = random.choice([
                "Immer wieder gerne",
                "Kann ich nur empfehlen",
                "Top Service",
                "Weiter so"
            ])
        else:
            closing = "War okay"
            
        # VollstÃ¤ndiger Text
        full_text = f"{opening}. {main_text}. {closing}."
        
        # Persona-Anpassungen
        full_text = self._apply_persona_style(full_text, persona)
        
        return full_text
        
    def _detect_sentiment(self, text: str) -> str:
        """
        Simple sentiment detection based on keywords.
        
        Args:
            text (str): Text to analyze
            
        Returns:
            str: Detected sentiment ('positiv', 'neutral', 'negativ')
        """
        positive_keywords = ['gut', 'super', 'toll', 'perfekt', 'zufrieden', 'empfehlen']
        negative_keywords = ['schlecht', 'enttÃ¤uscht', 'mangelhaft', 'problem', 'unzufrieden']
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_keywords if word in text_lower)
        neg_count = sum(1 for word in negative_keywords if word in text_lower)
        
        if pos_count > neg_count:
            return 'positiv'
        elif neg_count > pos_count:
            return 'negativ'
        else:
            return 'neutral'
            
    def _apply_temporal_effects(
        self, 
        date: datetime, 
        sentiment_score: float,
        topic: str
    ) -> float:
        """
        Applies temporal effects to sentiment.
        
        Args:
            date (datetime): Date for temporal context
            sentiment_score (float): Base sentiment score
            topic (str): Topic of the feedback
            
        Returns:
            float: Adjusted sentiment score normalized to [-1, 1]
        """
        
        # Saisonale Effekte
        month = date.month
        if month in [12, 1, 2]:  # Winter
            season = 'winter'
        elif month in [3, 4, 5]:  # FrÃ¼hling
            season = 'spring'
        elif month in [6, 7, 8]:  # Sommer
            season = 'summer'
        else:  # Herbst
            season = 'autumn'
            
        if topic in self.temporal_patterns['seasonal'][season]:
            sentiment_score += self.temporal_patterns['seasonal'][season][topic]
            
        # Wochentag-Effekt
        weekday = date.weekday()
        sentiment_score += self.temporal_patterns['weekday'][weekday]
        
        # Tageszeit-Effekt (simuliert)
        hour = random.randint(8, 20)
        if hour < 12:
            time_effect = self.temporal_patterns['time_of_day']['morning']['friendliness']
        elif hour < 14:
            time_effect = self.temporal_patterns['time_of_day']['noon']['friendliness']
        elif hour < 18:
            time_effect = self.temporal_patterns['time_of_day']['afternoon']['friendliness']
        else:
            time_effect = self.temporal_patterns['time_of_day']['evening']['friendliness']
            
        sentiment_score += time_effect * 0.1
        
        # Normalisieren auf [-1, 1]
        return max(-1, min(1, sentiment_score))
        
    def _calculate_realistic_nps_sentiment_correlation(
        self,
        nps_score: int,
        persona: PersonaProfile
    ) -> str:
        """
        Calculates realistic sentiment with persona influence.
        
        Args:
            nps_score (int): NPS score (0-10)
            persona (PersonaProfile): Persona profile for influence
            
        Returns:
            str: Calculated sentiment ('positiv', 'neutral', 'negativ')
        """
        
        # Basis-Wahrscheinlichkeiten
        if nps_score >= 9:  # Promoter
            base_probs = {'positiv': 0.7, 'neutral': 0.25, 'negativ': 0.05}
        elif nps_score <= 6:  # Detractor
            base_probs = {'positiv': 0.05, 'neutral': 0.25, 'negativ': 0.7}
        else:  # Passive
            base_probs = {'positiv': 0.2, 'neutral': 0.6, 'negativ': 0.2}
            
        # Persona-Anpassungen (mit Clipping um negative Werte zu vermeiden)
        if persona.communication_style == 'critical':
            base_probs['negativ'] = min(0.9, base_probs['negativ'] + 0.1)
            base_probs['positiv'] = max(0.0, base_probs['positiv'] - 0.1)
        elif persona.communication_style == 'friendly':
            base_probs['positiv'] = min(0.9, base_probs['positiv'] + 0.1)
            base_probs['negativ'] = max(0.0, base_probs['negativ'] - 0.1)
            
        # Normalisieren und sicherstellen dass alle Werte >= 0 sind
        for key in base_probs:
            base_probs[key] = max(0.0, base_probs[key])
        
        total = sum(base_probs.values())
        if total == 0:
            total = 1  # Fallback
        probs = [base_probs[s]/total for s in self.sentiments]
        
        return np.random.choice(self.sentiments, p=probs)
        
    def generate_enterprise_dataset(
        self,
        n_samples: int = 5000,
        start_date: str = '2020-01-01',
        end_date: str = '2024-12-31',
        ensure_diversity: bool = True,
        include_metadata: bool = True
    ) -> pd.DataFrame:
        """
        Generates enterprise-grade synthetic data.
        
        Args:
            n_samples (int): Number of records to generate. Defaults to 5000
            start_date (str): Start date in format 'YYYY-MM-DD'. Defaults to '2020-01-01'
            end_date (str): End date in format 'YYYY-MM-DD'. Defaults to '2024-12-31'
            ensure_diversity (bool): Enforce diversity controls. Defaults to True
            include_metadata (bool): Include metadata columns. Defaults to True
            
        Returns:
            pd.DataFrame: DataFrame with synthetic customer feedback data
        """
        
        print(f">> Generiere {n_samples} synthetische Datensaetze...")
        if ensure_diversity:
            print("   >> Aktiviere Diversitaets-Kontrolle...")
        print("   >> Anonymisierung aktiv...")
        
        data = []
        
        # Datum-Range
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        date_range = (end - start).days
        
        # DiversitÃ¤ts-Kontrolle - initialisiere Cycles (auch wenn nicht verwendet)
        personas_cycle = itertools.cycle(self.personas.keys())
        markets_cycle = itertools.cycle(self.markets)  # Verwende neue markets Liste
        topics_cycle = itertools.cycle(list(self.topics_hierarchy.keys()))
            
        for i in range(n_samples):
            # Progress indicator
            if i % 500 == 0 and i > 0:
                print(f"   >> {i}/{n_samples} Datensaetze generiert...")
                
            # Persona wÃ¤hlen
            if ensure_diversity:
                persona_name = next(personas_cycle)
                persona = self.personas[persona_name]
            else:
                persona_name, persona = self._select_persona()
                
            # NPS Score - MIT GEWICHTEN aus Analyse (16.8% / 27.4% / 55.8%)
            nps_category = np.random.choice(
                list(self.nps_weights.keys()),
                p=list(self.nps_weights.values())
            )
            nps_score = random.choice(self.nps_distribution[nps_category])
            
            # Market - KORREKTES FORMAT: Region-Country
            if ensure_diversity:
                market = next(markets_cycle)
            else:
                market = random.choice(self.markets)
            
            # Extrahiere Region und Country aus Market
            market_info = self.market_mapping[market]
            region = market_info['region']
            country = market_info['country']
                
            # Datum mit realistischer Verteilung
            # Mehr aktuelle Daten
            if random.random() < 0.3:  # 30% aus letztem Quartal
                days_ago = random.randint(0, 90)
            else:
                days_ago = random.randint(0, date_range)
            feedback_date = end - timedelta(days=days_ago)
            
            # Topic und Subtopic
            if ensure_diversity:
                topic = next(topics_cycle)
            else:
                topic = random.choice(list(self.topics_hierarchy.keys()))
            
            # NEU: Handle neue Topic-Struktur mit 'subtopics' und 'weight'
            topic_data = self.topics_hierarchy.get(topic, {})
            if isinstance(topic_data, dict):
                subtopics_list = topic_data.get('subtopics', [])
            else:
                # Fallback fÃ¼r alte Struktur
                subtopics_list = topic_data if isinstance(topic_data, list) else []
            
            subtopic = random.choice(subtopics_list) if subtopics_list else None
            
            # Sentiment (realistisch korreliert mit NPS)
            sentiment = self._calculate_realistic_nps_sentiment_correlation(
                nps_score, persona
            )
            
            # Generiere Text
            verbatim = self._generate_dynamic_feedback_text(
                topic, sentiment, persona, subtopic
            )
            
            # Sentiment Score berechnen
            base_sentiment_score = {
                'positiv': np.random.uniform(0.5, 1.0),
                'neutral': np.random.uniform(-0.2, 0.5),
                'negativ': np.random.uniform(-1.0, -0.3)
            }[sentiment]
            
            # Zeitliche Effekte anwenden
            sentiment_score = self._apply_temporal_effects(
                feedback_date, base_sentiment_score, topic
            )
            
            # Metadaten
            record = {
                # Kern-Daten
                'feedback_id': self._generate_session_id(),
                'customer_id': self._generate_customer_id(),
                'NPS': nps_score,
                'nps_category': nps_category,
                'Market': market,
                'Date': feedback_date.strftime('%Y-%m-%dT%H:%M:%S+00:00'),
                'Verbatim': verbatim,
                
                # Erweiterte Daten - verwende bereits extrahierte Werte
                'region': region,
                'country': country,
                'topic': topic,
                'subtopic': subtopic,
                'sentiment_label': sentiment,
                'sentiment_score': round(sentiment_score, 4),
                
                # Text-Metriken
                'Verbatim_token_count': len(verbatim.split()),
                'Verbatim_char_count': len(verbatim),
                
                # Tracking
                'dealership': random.choice(self.fake_dealerships),
                'response_channel': random.choice(['Online', 'Email', 'App', 'Phone', 'In-Person']),
                'survey_type': random.choice(['Post-Service', 'Annual', 'Transaction', 'Complaint']),
                
                # Quality Flags
                'is_synthetic': True,  # Wichtig fÃ¼r Mixing mit echten Daten
                'synthetic_version': '2.0',
                'generation_timestamp': datetime.now().isoformat()
            }
            
            # ZusÃ¤tzliche Metadaten wenn gewÃ¼nscht
            if include_metadata:
                record.update({
                    'persona_type': persona_name,
                    'age_group': persona.age_group,
                    'tech_affinity': persona.tech_affinity,
                    'communication_style': persona.communication_style,
                    'topic_confidence': round(random.uniform(0.3, 1.0), 2),
                    'response_time_seconds': random.randint(30, 600),
                    'device_type': random.choice(['Desktop', 'Mobile', 'Tablet']),
                    'browser': random.choice(['Chrome', 'Safari', 'Firefox', 'Edge']),
                    'city': random.choice(self.fake_cities)
                })
                
            data.append(record)
            
        df = pd.DataFrame(data)
        
        # Sortiere nach Datum
        df = df.sort_values('Date').reset_index(drop=True)
        
        print(f">> Erfolgreich {len(df)} Datensaetze generiert!")
        
        # QualitÃ¤tskontrolle
        self._run_quality_checks(df)
        
        return df
        
    def _run_quality_checks(self, df: pd.DataFrame):
        """
        Runs quality checks on generated data.
        
        Args:
            df (pd.DataFrame): DataFrame to check
            
        Returns:
            None
        """
        print("\n>> Qualitaetskontrolle:")
        
        # Check 1: DiversitÃ¤t
        unique_verbatims = df['Verbatim'].nunique()
        diversity_score = unique_verbatims / len(df)
        print(f"   >> Text-Diversitaet: {diversity_score:.2%} unique")
        
        # Check 2: Balance
        nps_balance = df['nps_category'].value_counts(normalize=True)
        print(f"   âœ“ NPS-Balance: {nps_balance.to_dict()}")
        
        # Check 3: Sentiment-Korrelation
        correlation = df['NPS'].corr(df['sentiment_score'])
        print(f"   âœ“ NPS-Sentiment Korrelation: {correlation:.3f}")
        
        # Check 4: Market-Verteilung
        market_distribution = df['Market'].value_counts()
        cv = market_distribution.std() / market_distribution.mean()
        print(f"   âœ“ Market-Gleichverteilung CV: {cv:.3f} (niedriger=besser)")
        
        # Check 5: Keine leeren Werte
        null_check = df.isnull().sum().sum()
        print(f"   âœ“ Fehlende Werte: {null_check}")
        
        if diversity_score < 0.8:
            print("   âš ï¸  Warnung: Text-DiversitÃ¤t unter 80%")
        if cv > 0.5:
            print("   âš ï¸  Warnung: Market-Verteilung ungleichmÃ¤ÃŸig")
            
    def analyze_bias_advanced(self, df: pd.DataFrame) -> Dict:
        """
        Advanced bias analysis with statistical tests.
        
        Args:
            df (pd.DataFrame): DataFrame to analyze
            
        Returns:
            Dict: Dictionary containing basic distributions, statistical tests,
                  diversity metrics, and bias indicators
        """
        
        from scipy import stats
        
        print("\nðŸ“ˆ Erweiterte Bias-Analyse lÃ¤uft...")
        
        analysis = {
            'basic_distributions': {
                'nps': df['NPS'].value_counts().to_dict(),
                'sentiment': df['sentiment_label'].value_counts(normalize=True).to_dict(),
                'topics': df['topic'].value_counts(normalize=True).to_dict()
            },
            'statistical_tests': {},
            'diversity_metrics': {},
            'bias_indicators': {}
        }
        
        # Chi-Quadrat Tests fÃ¼r UnabhÃ¤ngigkeit
        categorical_pairs = [
            ('Market', 'sentiment_label'),
            ('Market', 'nps_category'),
            ('topic', 'sentiment_label')
        ]
        
        for var1, var2 in categorical_pairs:
            try:
                contingency_table = pd.crosstab(df[var1], df[var2])
                chi2_stat, p_val, _, _ = stats.chi2_contingency(contingency_table)
                chi2_stat = float(chi2_stat) # type: ignore
                p_val = float(p_val) # type: ignore
                
                analysis['statistical_tests'][f'{var1}_vs_{var2}'] = {
                    'chi2': round(chi2_stat, 4),
                    'p_value': round(p_val, 4),
                    'independent': p_val > 0.05,
                    'interpretation': 'Variables are independent' if p_val > 0.05 else 'Significant dependency detected'
                }
            except Exception as e:
                analysis['statistical_tests'][f'{var1}_vs_{var2}'] = {
                    'error': str(e)
                }
            
        # Shannon Entropy fÃ¼r DiversitÃ¤t
        from scipy.stats import entropy
        
        for column in ['Market', 'topic', 'sentiment_label']:
            value_counts = df[column].value_counts(normalize=True)
            shannon_entropy = entropy(value_counts)
            max_entropy = np.log(len(value_counts))
            normalized_entropy = shannon_entropy / max_entropy if max_entropy > 0 else 0
            
            analysis['diversity_metrics'][column] = {
                'shannon_entropy': round(float(shannon_entropy), 4),
                'normalized_entropy': round(float(normalized_entropy), 4),
                'interpretation': 'High diversity' if normalized_entropy > 0.8 else 'Low diversity'
            }
            
        # Bias Indicators
        # Gender representation in employee names (wenn vorhanden)
        if 'Verbatim' in df.columns:
            male_mentions = df['Verbatim'].str.contains('Herr', case=False).sum()
            female_mentions = df['Verbatim'].str.contains('Frau', case=False).sum()
            
            total_gendered = male_mentions + female_mentions
            if total_gendered > 0:
                analysis['bias_indicators']['gender_balance'] = {
                    'male_ratio': round(male_mentions / total_gendered, 3),
                    'female_ratio': round(female_mentions / total_gendered, 3),
                    'balanced': abs(0.5 - male_mentions/total_gendered) < 0.1
                }
                
        return analysis


def main():
    """
    Main function with extended features.
    
    Returns:
        None
    """
    
    print("="*60)
    print("ðŸš€ ENTERPRISE SYNTHETIC FEEDBACK GENERATOR v2.0")
    print("="*60)
    
    # Initialisierung
    generator = AdvancedSyntheticFeedbackGenerator(seed=42, enable_fun_mode=True)
    
    # Generiere groÃŸe Datenmenge
    print("\nðŸ“Š Phase 1: Generierung groÃŸer Datenmenge")
    df_large = generator.generate_enterprise_dataset(
        n_samples=5000,
        start_date='2022-01-01',
        end_date='2024-12-31',
        ensure_diversity=True,
        include_metadata=True
    )
    
    # Erweiterte Analyse
    print("\nðŸ“Š Phase 2: Erweiterte Bias-Analyse")
    bias_analysis = generator.analyze_bias_advanced(df_large)
    
    print("\nðŸ“ˆ Analyse-Ergebnisse:")
    print(f"   â€¢ Sentiment-Verteilung: {bias_analysis['basic_distributions']['sentiment']}")
    
    for test, result in bias_analysis['statistical_tests'].items():
        print(f"   â€¢ {test}: {result['interpretation']} (p={result['p_value']:.4f})")
        
    for metric, result in bias_analysis['diversity_metrics'].items():
        print(f"   â€¢ {metric} DiversitÃ¤t: {result['interpretation']} (Entropy={result['normalized_entropy']:.2f})")
    
    # Beispiel-Output
    print("\nðŸ“„ Beispiel-DatensÃ¤tze:")
    print("="*60)
    
    # Zeige verschiedene Personas
    for persona_type in ['digital_native', 'experienced_senior', 'busy_professional']:
        sample = df_large[df_large['persona_type'] == persona_type].iloc[0]
        print(f"\nðŸ‘¤ Persona: {persona_type}")
        print(f"   NPS: {sample['NPS']} ({sample['nps_category']})")
        print(f"   Topic: {sample['topic']} - {sample['subtopic']}")
        print(f"   Sentiment: {sample['sentiment_label']} ({sample['sentiment_score']:.2f})")
        print(f"   Werkstatt: {sample['dealership']}")
        print(f"   Feedback: \"{sample['Verbatim'][:150]}...\"")
    
    print("\n="*60)
    print("âœ… GENERIERUNG ERFOLGREICH ABGESCHLOSSEN!")
    print(f"   â€¢ {len(df_large)} DatensÃ¤tze generiert")
    print(f"   â€¢ {df_large['Verbatim'].nunique()} unique Texte")
    print(f"   â€¢ {df_large['dealership'].nunique()} verschiedene WerkstÃ¤tten")
    print("="*60)


if __name__ == "__main__":
    main()